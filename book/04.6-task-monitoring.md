# Chapter 4.6 - Task Completion Verification

## Introduction

When agents run unattended, a critical question emerges: **Has it actually finished the work?** Without verification, you risk agents that prematurely declare victory with incomplete deliverables.

WINK provides **Task Completion Checking**: a mechanism that verifies all assigned tasks are complete before allowing the agent to stop. This enables reliable unattended execution with built-in safeguards against premature termination.

This chapter covers task completion verification in depth, showing how to:

- Implement completion verification using built-in and custom checkers
- Combine multiple verification strategies
- Test completion logic in isolation
- Deploy completion checking for production agents

## Task Completion Verification

### The Problem: Premature Termination

Consider an agent tasked with "refactoring the authentication module, updating tests, and running the full test suite." Without completion verification, the agent might:

1. Refactor the code
1. Update a few tests
1. Call `StructuredOutput` with a summary
1. Stop before running the test suite

The work appears complete from the agent's perspective, but critical steps remain unfinished.

Task completion checking solves this by **verifying task state before allowing termination**. If tasks remain incomplete, the agent receives explicit feedback and continues working.

### Architecture Overview

```mermaid
flowchart TB
    subgraph Agent["Agent Execution"]
        Work["Agent performs work"]
        Output["Agent calls StructuredOutput"]
    end

    subgraph Checker["Task Completion Checker"]
        Context["Build TaskCompletionContext"]
        Verify["checker.check(context)"]
        Result["TaskCompletionResult"]
    end

    subgraph Decision["Termination Decision"]
        Complete{"complete == True?"}
        Stop["Allow stop, return output"]
        Feedback["Inject feedback via hook"]
        Continue["Signal continuation"]
    end

    Work --> Output
    Output --> Context
    Context --> Verify
    Verify --> Result
    Result --> Complete
    Complete -->|Yes| Stop
    Complete -->|No| Feedback
    Feedback --> Continue
    Continue --> Work
```

The checker runs at three points in the Claude Agent SDK adapter:

1. **PostToolUse Hook**: After `StructuredOutput` executes, verify completion before allowing the output to be captured
1. **Stop Hook**: Before allowing the agent to stop for other reasons (e.g., `end_turn`), verify completion
1. **Final Verification**: After SDK completes, verify completion before returning structured output to the caller

### The CompletionChecker Protocol

Task completion checkers implement a simple protocol:

```python
from typing import Protocol, runtime_checkable, Any
from dataclasses import dataclass

@dataclass(slots=True)
class TaskCompletionContext:
    """Context provided to checkers for evaluation.

    Attributes:
        session: Session containing state slices for verification
        tentative_output: Output being produced (StructuredOutput payload)
        filesystem: Optional filesystem access for file verification
        adapter: Optional adapter for LLM-based verification
        stop_reason: Why the agent is attempting to stop
    """
    session: Session
    tentative_output: Any = None
    filesystem: Filesystem | None = None
    adapter: ProviderAdapter | None = None
    stop_reason: str | None = None


@dataclass(frozen=True)
class TaskCompletionResult:
    """Result of completion verification.

    Attributes:
        complete: Whether all tasks are complete
        feedback: Natural language explanation of incompleteness
    """
    complete: bool
    feedback: str | None = None

    @classmethod
    def ok(cls, feedback: str | None = None) -> "TaskCompletionResult":
        """Tasks are complete."""
        return cls(complete=True, feedback=feedback)

    @classmethod
    def incomplete(cls, feedback: str) -> "TaskCompletionResult":
        """Tasks remain incomplete."""
        return cls(complete=False, feedback=feedback)


@runtime_checkable
class TaskCompletionChecker(Protocol):
    """Protocol for task completion verification."""

    def check(self, context: TaskCompletionContext) -> TaskCompletionResult:
        """Verify task completion.

        Args:
            context: Context with session state, output, and resources

        Returns:
            Result indicating completion status with feedback
        """
        ...
```

The protocol is deliberately minimal. Any object with a `check` method satisfies the interface.

### PlanBasedChecker: Verifying Plan State

The most common completion pattern is **plan-based verification**: the agent creates a plan with explicit tasks, then marks tasks complete as work progresses. `PlanBasedChecker` enforces that all plan steps reach `status == "done"` before allowing termination.

```python
from weakincentives.adapters.claude_agent_sdk import (
    ClaudeAgentSDKAdapter,
    ClaudeAgentSDKClientConfig,
    PlanBasedChecker,
)
from weakincentives.contrib.tools.planning import Plan
from weakincentives.contrib.tools import PlanningToolsSection
from weakincentives.prompt import Prompt, PromptTemplate, MarkdownSection
from weakincentives.runtime import Session, InProcessDispatcher

# Create session with planning tools
session = Session(bus=InProcessDispatcher())

template = PromptTemplate[None](
    ns="code-review",
    key="main",
    name="code_review_prompt",
    sections=[
        MarkdownSection(
            title="Task",
            template="""
You are a code reviewer. Create a plan with these steps:
1. Read the changed files
2. Analyze for issues (bugs, style, performance)
3. Write review comments
4. Run linter and tests
5. Produce final review report

Mark each step as "done" when complete.
            """,
            key="task",
        ),
        PlanningToolsSection(session=session),
    ],
)

# Configure adapter with plan-based completion checking
adapter = ClaudeAgentSDKAdapter(
    client_config=ClaudeAgentSDKClientConfig(
        task_completion_checker=PlanBasedChecker(plan_type=Plan),
    ),
)

# Agent will not terminate until all plan steps are marked "done"
response = adapter.evaluate(Prompt(template), session=session)
```

**Behavior:**

- Returns `ok()` if `Plan` slice is not registered (no plan configured)
- Returns `ok()` if no plan has been created yet (nothing to enforce)
- Returns `ok()` if all plan steps have `status == "done"`
- Returns `incomplete()` with detailed feedback listing incomplete tasks otherwise

**Feedback Example:**

```
You have 2 incomplete task(s) out of 5. Please either complete all remaining
tasks or update the plan to remove tasks that are no longer needed before
producing output: Run linter and tests, Produce final review report...
```

The feedback explicitly guides the agent: either finish the work or update the plan to remove tasks that are no longer relevant.

### CompositeChecker: Multi-Strategy Verification

`CompositeChecker` combines multiple verification strategies with configurable logic:

```python
from weakincentives.adapters.claude_agent_sdk import CompositeChecker

# All checkers must pass (AND logic)
checker = CompositeChecker(
    checkers=(
        PlanBasedChecker(plan_type=Plan),
        TestPassingChecker(),  # Custom checker (see below)
    ),
    all_must_pass=True,  # Default behavior
)

# Any checker can pass (OR logic)
checker = CompositeChecker(
    checkers=(
        PlanBasedChecker(plan_type=Plan),
        FileExistsChecker(required_files=("output.json",)),
    ),
    all_must_pass=False,
)
```

**AND Logic** (`all_must_pass=True`):

- Short-circuits on first failure, returning that result
- If all pass, combines feedback from all checkers
- Use for **required conditions**: plan complete AND tests passing AND files exist

**OR Logic** (`all_must_pass=False`):

- Short-circuits on first success, returning that result
- If all fail, combines feedback from all failing checkers
- Use for **alternative success criteria**: plan complete OR output file exists

### Custom Completion Checkers

Implement custom verification logic by creating a class with a `check` method:

```python
from dataclasses import dataclass

@dataclass(frozen=True)
class TestResult:
    """Session state tracking test execution."""
    passed: int
    failed: int
    total: int


class TestPassingChecker:
    """Verify all tests pass before allowing completion."""

    def check(self, context: TaskCompletionContext) -> TaskCompletionResult:
        # Query session state
        test_results = context.session[TestResult].latest()

        if test_results is None:
            return TaskCompletionResult.incomplete(
                "No test results found. Please run the test suite before "
                "marking tasks complete."
            )

        if test_results.failed > 0:
            return TaskCompletionResult.incomplete(
                f"{test_results.failed} test(s) failing. Fix failures before "
                f"completing. ({test_results.passed} passing)"
            )

        return TaskCompletionResult.ok(
            f"All {test_results.passed} tests passing."
        )
```

**File-Based Verification:**

```python
class FileExistsChecker:
    """Verify required files exist before completion."""

    def __init__(self, required_files: tuple[str, ...]) -> None:
        self._required = required_files

    def check(self, context: TaskCompletionContext) -> TaskCompletionResult:
        if context.filesystem is None:
            return TaskCompletionResult.ok(
                "No filesystem configured; skipping file checks."
            )

        missing = [
            f for f in self._required
            if not context.filesystem.exists(f)
        ]

        if missing:
            return TaskCompletionResult.incomplete(
                f"Missing required output files: {', '.join(missing)}. "
                f"Please generate all required outputs before completing."
            )

        return TaskCompletionResult.ok(
            f"All {len(self._required)} required files present."
        )
```

### Integration with Claude Agent SDK

Task completion checking is **disabled by default**. Enable it by configuring a checker via `ClaudeAgentSDKClientConfig`:

```python
from weakincentives.adapters.claude_agent_sdk import (
    ClaudeAgentSDKAdapter,
    ClaudeAgentSDKClientConfig,
    PlanBasedChecker,
)

adapter = ClaudeAgentSDKAdapter(
    client_config=ClaudeAgentSDKClientConfig(
        task_completion_checker=PlanBasedChecker(plan_type=Plan),
    ),
)
```

**Hook Integration Points:**

```mermaid
sequenceDiagram
    participant Agent
    participant SDK as Claude SDK
    participant Hook as PostToolUse Hook
    participant Checker as TaskCompletionChecker

    Agent->>SDK: StructuredOutput(...)
    SDK->>Hook: PostToolUse event
    Hook->>Checker: check(context)

    alt Tasks incomplete
        Checker-->>Hook: incomplete=True, feedback
        Hook-->>SDK: additionalContext: feedback
        SDK-->>Agent: Continue with feedback
    else Tasks complete
        Checker-->>Hook: complete=True
        Hook-->>SDK: continue=False
        SDK-->>Agent: Allow output capture
    end
```

**Three Verification Points:**

1. **PostToolUse Hook (StructuredOutput)**: When the agent calls `StructuredOutput`, the hook checks completion. If incomplete, adds `additionalContext` with feedback encouraging continuation.

1. **Stop Hook**: When the agent attempts to stop for other reasons (e.g., `end_turn`), the hook checks completion. If incomplete, returns `{"needsMoreTurns": True, "decision": "continue"}`.

1. **Final Verification**: After the SDK completes, the adapter performs a final completion check before returning output. If incomplete, raises `PromptEvaluationError`. This catches edge cases where the SDK captured output before hooks could intervene.

**Budget and Deadline Bypass:**

When the budget is exhausted or the deadline has expired, completion checking is **skipped**. The agent cannot do more work, so forcing continuation would cause an infinite loop.

## Testing Strategies

### Unit Testing Checkers

Test checkers in isolation with mock contexts:

```python
from weakincentives.runtime import Session, InProcessDispatcher
from weakincentives.filesystem import InMemoryFilesystem
from weakincentives.adapters.claude_agent_sdk import (
    TaskCompletionContext,
    TaskCompletionResult,
)

def test_file_checker_detects_missing_files():
    """FileExistsChecker returns incomplete when files missing."""
    session = Session(bus=InProcessDispatcher())
    fs = InMemoryFilesystem()

    context = TaskCompletionContext(
        session=session,
        filesystem=fs,
    )

    checker = FileExistsChecker(required_files=("output.json", "summary.txt"))
    result = checker.check(context)

    assert result.complete is False
    assert "output.json" in result.feedback
    assert "summary.txt" in result.feedback


def test_file_checker_passes_when_files_exist():
    """FileExistsChecker returns ok when all files exist."""
    session = Session(bus=InProcessDispatcher())
    fs = InMemoryFilesystem()
    fs.write("output.json", "{}")
    fs.write("summary.txt", "Done")

    context = TaskCompletionContext(
        session=session,
        filesystem=fs,
    )

    checker = FileExistsChecker(required_files=("output.json", "summary.txt"))
    result = checker.check(context)

    assert result.complete is True
```

### Integration Testing

Test the full flow with real adapters:

```python
import pytest
from weakincentives.adapters.claude_agent_sdk import ClaudeAgentSDKAdapter

@pytest.mark.integration
def test_incomplete_plan_prevents_termination():
    """Agent continues when plan is incomplete."""
    session = Session(bus=InProcessDispatcher())

    # Seed incomplete plan
    plan = Plan(steps=(
        Step(id="1", title="Task 1", status="done"),
        Step(id="2", title="Task 2", status="pending"),
    ))
    session[Plan].seed(plan)

    template = PromptTemplate[None](
        ns="test",
        key="test",
        sections=[
            MarkdownSection(
                title="Task",
                template="Complete all tasks, then call StructuredOutput.",
                key="task",
            ),
        ],
    )

    adapter = ClaudeAgentSDKAdapter(
        client_config=ClaudeAgentSDKClientConfig(
            task_completion_checker=PlanBasedChecker(plan_type=Plan),
        ),
    )

    # This should raise if tasks incomplete (depending on final verification)
    # or should result in additional turns
    with pytest.raises(PromptEvaluationError, match="incomplete"):
        adapter.evaluate(Prompt(template), session=session)
```

## Production Deployment Patterns

### Monitoring Completion Checks

Track how often agents hit completion checks to identify workflow issues:

```python
import structlog

logger = structlog.get_logger()

def log_completion_check(
    result: TaskCompletionResult,
    session_id: str,
    call_count: int,
) -> None:
    """Log completion check for monitoring."""
    logger.info(
        "task_completion_check",
        session_id=session_id,
        complete=result.complete,
        feedback=result.feedback,
        tool_calls=call_count,
    )
```

### Real-time Status Dashboard

Expose completion status via REST API:

```python
from flask import Flask, jsonify
from weakincentives.runtime import Session

app = Flask(__name__)

@app.route("/api/sessions/<session_id>/status")
def get_status(session_id: str):
    """Return current session status."""
    session = session_registry.get(session_id)

    # Check plan completion
    plan = session[Plan].latest()
    plan_status = {
        "total_steps": len(plan.steps) if plan else 0,
        "completed_steps": len([s for s in plan.steps if s.status == "done"]) if plan else 0,
    }

    return jsonify({
        "session_id": session_id,
        "tool_calls": len(session[ToolInvoked].all()),
        "plan": plan_status,
    })
```

### Alerting on Repeated Failures

Alert when agents repeatedly fail completion checks:

```python
def alert_on_repeated_failures(session: Session, session_id: str) -> None:
    """Alert if agent is stuck at completion check."""
    tool_calls = len(session[ToolInvoked].all())

    # If agent has made many calls but still incomplete, alert
    if tool_calls > 100:
        slack_client.post_message(
            channel="#agent-alerts",
            text=f"⚠️ Agent {session_id} has made {tool_calls} tool calls "
                 f"but tasks remain incomplete. May be stuck.",
        )
```

## Best Practices

### Completion Checking

1. **Enable by default for unattended agents**: Don't rely on the agent to self-terminate correctly.

1. **Use CompositeChecker for multi-faceted verification**: Combine plan completion with domain-specific checks (tests passing, files generated, etc.).

1. **Provide actionable feedback**: Guide the agent on how to complete remaining work or update the plan.

1. **Bypass on budget exhaustion**: Task completion checking automatically skips when the budget is exhausted to avoid infinite loops.

1. **Test checkers in isolation**: Mock the `TaskCompletionContext` to test verification logic without running full agent sessions.

1. **Log completion checks**: Track completion check frequency and failure patterns to identify workflow issues.

1. **Monitor for stuck agents**: Alert when agents make many tool calls without completing tasks.

### Combining Multiple Checkers

1. **Use AND logic for required conditions**: All conditions must be satisfied (plan complete AND tests passing AND files exist).

1. **Use OR logic for alternative success criteria**: Any condition can satisfy completion (plan complete OR output file exists).

1. **Order checkers by cost**: In CompositeChecker with AND logic, put cheapest checks first to fail fast.

1. **Share state via session slices**: All checkers query the same session state, ensuring consistency.

## Advanced: LLM-as-Judge Verification (Future)

A future enhancement would enable **LLM-based completion verification** for subjective criteria:

```python
# Future API - not yet implemented
class LLMJudgeChecker:
    """Use LLM to evaluate task completion against criteria."""

    def __init__(
        self,
        *,
        criteria: str,
        adapter: ProviderAdapter,
    ) -> None:
        self._criteria = criteria
        self._adapter = adapter

    def check(self, context: TaskCompletionContext) -> TaskCompletionResult:
        # Build verification prompt
        verification_prompt = f"""
Review the following output against these criteria:

{self._criteria}

Output to evaluate:
{context.tentative_output}

Session state:
{self._serialize_session(context.session)}

Respond with:
- "COMPLETE" if all criteria are met
- "INCOMPLETE: <reason>" if criteria are not met
"""

        # Call LLM for judgment
        response = self._adapter.evaluate(verification_prompt)

        if response.startswith("COMPLETE"):
            return TaskCompletionResult.ok("LLM verified completion")
        else:
            reason = response.replace("INCOMPLETE:", "").strip()
            return TaskCompletionResult.incomplete(reason)
```

This is deferred until the use case is validated in production scenarios.

## Conclusion

Task completion verification is essential for reliable unattended agent execution. By implementing **task completion checkers** as hard gates before termination, you create agents that:

- **Complete all assigned work** before terminating
- **Receive actionable feedback** when tasks remain incomplete
- **Avoid premature termination** that leaves work unfinished
- **Provide visibility** into completion status

Start with `PlanBasedChecker` for plan-based agents, then expand with custom verification logic tailored to your domain. Use `CompositeChecker` to combine multiple verification strategies with AND/OR logic. Test completion logic in isolation, deploy with structured logging and monitoring, and iterate based on observed agent behavior in production.
